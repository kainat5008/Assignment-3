{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXDiyMQC5Ve4",
        "outputId": "fb19603b-8a17-423d-be2a-404a450a0d3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.24.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy pandas matplotlib scikit-learn torch wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing, load_iris\n",
        "import pandas as pd\n",
        "\n",
        "# Load datasets\n",
        "california = fetch_california_housing()\n",
        "iris = load_iris()\n",
        "\n",
        "# Convert to DataFrames (for easier handling)\n",
        "california_df = pd.DataFrame(california.data, columns=california.feature_names)\n",
        "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "\n",
        "print(\"California Housing Dataset:\\n\", california_df.head())\n",
        "print(\"\\nIris Dataset (Classification):\\n\", iris_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ru6hh94a53vw",
        "outputId": "53cfbdc4-b153-47d7-af8a-54acc2e111e7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "California Housing Dataset:\n",
            "    MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
            "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
            "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
            "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
            "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
            "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
            "\n",
            "   Longitude  \n",
            "0    -122.23  \n",
            "1    -122.22  \n",
            "2    -122.24  \n",
            "3    -122.25  \n",
            "4    -122.25  \n",
            "\n",
            "Iris Dataset (Classification):\n",
            "    sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
            "0                5.1               3.5                1.4               0.2\n",
            "1                4.9               3.0                1.4               0.2\n",
            "2                4.7               3.2                1.3               0.2\n",
            "3                4.6               3.1                1.5               0.2\n",
            "4                5.0               3.6                1.4               0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Prepare data\n",
        "X = california.data\n",
        "y = california.target\n",
        "\n",
        "# Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features (critical for Gradient Descent)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Add bias term (intercept)\n",
        "X_train = np.c_[np.ones(X_train.shape[0]), X_train]\n",
        "X_test = np.c_[np.ones(X_test.shape[0]), X_test]"
      ],
      "metadata": {
        "id": "47nE9yI1AYMr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_gradient_descent(X, y, learning_rate=0.01, epochs=1000):\n",
        "    theta = np.zeros(X.shape[1])  # Initialize weights\n",
        "    m = len(y)                   # Number of samples\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        gradient = (1/m) * X.T.dot(X.dot(theta) - y)\n",
        "        theta -= learning_rate * gradient\n",
        "        if epoch % 100 == 0:\n",
        "            loss = np.mean((X.dot(theta) - y) ** 2)\n",
        "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "    return theta\n",
        "\n",
        "theta_batch = batch_gradient_descent(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12TMoplcAit1",
        "outputId": "fa4f9d20-dd4c-4c5a-c70d-27e9dfe21146"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 5.5300\n",
            "Epoch 100, Loss: 1.2906\n",
            "Epoch 200, Loss: 0.7102\n",
            "Epoch 300, Loss: 0.6187\n",
            "Epoch 400, Loss: 0.5952\n",
            "Epoch 500, Loss: 0.5827\n",
            "Epoch 600, Loss: 0.5732\n",
            "Epoch 700, Loss: 0.5652\n",
            "Epoch 800, Loss: 0.5584\n",
            "Epoch 900, Loss: 0.5526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def stochastic_gradient_descent(X, y, learning_rate=0.01, epochs=10):\n",
        "    theta = np.zeros(X.shape[1])\n",
        "    m = len(y)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for i in range(m):\n",
        "            random_idx = np.random.randint(m)\n",
        "            xi = X[random_idx:random_idx+1]\n",
        "            yi = y[random_idx:random_idx+1]\n",
        "            gradient = xi.T.dot(xi.dot(theta) - yi)\n",
        "            theta -= learning_rate * gradient\n",
        "        loss = np.mean((X.dot(theta) - y) ** 2)\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "    return theta\n",
        "\n",
        "theta_sgd = stochastic_gradient_descent(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zF7Xu7XSArc3",
        "outputId": "390b1a97-f78b-4b79-fabf-464ee2d0c5d9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 83089740333437.4219\n",
            "Epoch 1, Loss: 2709436668474954240.0000\n",
            "Epoch 2, Loss: 215500783293900110521760038277611520.0000\n",
            "Epoch 3, Loss: 5995712062525167513402413664527320642922090384236612681728.0000\n",
            "Epoch 4, Loss: 2484570317900802399534651676835908119936723980243246134088171520.0000\n",
            "Epoch 5, Loss: 930383876453167826657065601701807422264932130995711307430726734543295067717632.0000\n",
            "Epoch 6, Loss: 30048321488197100396717198290278676547118724460425136997438895340525623095026107023360.0000\n",
            "Epoch 7, Loss: 7871242120461777333456377121059425376044538412441013149239536872363890748914736792247331717120.0000\n",
            "Epoch 8, Loss: 629935055275031152277882001235560250448982095580445708685398469455531333000268831005204092876749327313141760.0000\n",
            "Epoch 9, Loss: 714130612631934034727119933884755492635571242658701894496049168094613467488996845844532227870031599862950396816451843391488.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mini_batch_gradient_descent(X, y, learning_rate=0.01, epochs=100, batch_size=32):\n",
        "    theta = np.zeros(X.shape[1])\n",
        "    m = len(y)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        shuffled_indices = np.random.permutation(m)\n",
        "        X_shuffled = X[shuffled_indices]\n",
        "        y_shuffled = y[shuffled_indices]\n",
        "\n",
        "        for i in range(0, m, batch_size):\n",
        "            xi = X_shuffled[i:i+batch_size]\n",
        "            yi = y_shuffled[i:i+batch_size]\n",
        "            gradient = (1/batch_size) * xi.T.dot(xi.dot(theta) - yi)\n",
        "            theta -= learning_rate * gradient\n",
        "        loss = np.mean((X.dot(theta) - y) ** 2)\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "    return theta\n",
        "\n",
        "theta_minibatch = mini_batch_gradient_descent(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWj1ragpAuiQ",
        "outputId": "a4b8eb4a-957c-4bc2-9146-d377d32660e2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.5829\n",
            "Epoch 1, Loss: 0.5464\n",
            "Epoch 2, Loss: 0.5327\n",
            "Epoch 3, Loss: 0.5262\n",
            "Epoch 4, Loss: 0.5240\n",
            "Epoch 5, Loss: 0.5200\n",
            "Epoch 6, Loss: 0.5250\n",
            "Epoch 7, Loss: 0.5228\n",
            "Epoch 8, Loss: 0.5408\n",
            "Epoch 9, Loss: 0.5187\n",
            "Epoch 10, Loss: 0.5281\n",
            "Epoch 11, Loss: 0.5196\n",
            "Epoch 12, Loss: 0.5211\n",
            "Epoch 13, Loss: 0.5198\n",
            "Epoch 14, Loss: 0.5182\n",
            "Epoch 15, Loss: 0.5189\n",
            "Epoch 16, Loss: 0.5186\n",
            "Epoch 17, Loss: 0.5278\n",
            "Epoch 18, Loss: 0.5190\n",
            "Epoch 19, Loss: 0.5210\n",
            "Epoch 20, Loss: 0.5192\n",
            "Epoch 21, Loss: 0.5197\n",
            "Epoch 22, Loss: 0.5200\n",
            "Epoch 23, Loss: 0.5205\n",
            "Epoch 24, Loss: 0.5181\n",
            "Epoch 25, Loss: 0.5211\n",
            "Epoch 26, Loss: 0.5198\n",
            "Epoch 27, Loss: 0.5287\n",
            "Epoch 28, Loss: 0.5185\n",
            "Epoch 29, Loss: 0.5241\n",
            "Epoch 30, Loss: 0.5272\n",
            "Epoch 31, Loss: 0.5189\n",
            "Epoch 32, Loss: 0.5183\n",
            "Epoch 33, Loss: 0.5184\n",
            "Epoch 34, Loss: 0.5228\n",
            "Epoch 35, Loss: 0.5221\n",
            "Epoch 36, Loss: 0.5216\n",
            "Epoch 37, Loss: 0.5187\n",
            "Epoch 38, Loss: 0.5193\n",
            "Epoch 39, Loss: 0.5199\n",
            "Epoch 40, Loss: 0.5291\n",
            "Epoch 41, Loss: 0.5198\n",
            "Epoch 42, Loss: 0.5256\n",
            "Epoch 43, Loss: 0.5200\n",
            "Epoch 44, Loss: 0.5185\n",
            "Epoch 45, Loss: 0.5302\n",
            "Epoch 46, Loss: 0.5282\n",
            "Epoch 47, Loss: 0.5182\n",
            "Epoch 48, Loss: 0.5190\n",
            "Epoch 49, Loss: 0.5187\n",
            "Epoch 50, Loss: 0.5218\n",
            "Epoch 51, Loss: 0.5186\n",
            "Epoch 52, Loss: 0.5324\n",
            "Epoch 53, Loss: 0.5186\n",
            "Epoch 54, Loss: 0.5186\n",
            "Epoch 55, Loss: 0.5187\n",
            "Epoch 56, Loss: 0.5192\n",
            "Epoch 57, Loss: 0.5185\n",
            "Epoch 58, Loss: 0.5210\n",
            "Epoch 59, Loss: 0.5269\n",
            "Epoch 60, Loss: 0.5195\n",
            "Epoch 61, Loss: 0.5270\n",
            "Epoch 62, Loss: 0.5184\n",
            "Epoch 63, Loss: 0.5267\n",
            "Epoch 64, Loss: 0.5192\n",
            "Epoch 65, Loss: 0.5198\n",
            "Epoch 66, Loss: 0.5191\n",
            "Epoch 67, Loss: 0.5191\n",
            "Epoch 68, Loss: 0.5195\n",
            "Epoch 69, Loss: 0.5189\n",
            "Epoch 70, Loss: 0.5276\n",
            "Epoch 71, Loss: 0.5247\n",
            "Epoch 72, Loss: 0.5203\n",
            "Epoch 73, Loss: 0.5238\n",
            "Epoch 74, Loss: 0.5196\n",
            "Epoch 75, Loss: 0.5181\n",
            "Epoch 76, Loss: 0.5201\n",
            "Epoch 77, Loss: 0.5182\n",
            "Epoch 78, Loss: 0.5187\n",
            "Epoch 79, Loss: 0.5195\n",
            "Epoch 80, Loss: 0.5264\n",
            "Epoch 81, Loss: 0.5197\n",
            "Epoch 82, Loss: 0.5220\n",
            "Epoch 83, Loss: 0.5215\n",
            "Epoch 84, Loss: 0.5191\n",
            "Epoch 85, Loss: 0.5255\n",
            "Epoch 86, Loss: 0.5211\n",
            "Epoch 87, Loss: 0.5212\n",
            "Epoch 88, Loss: 0.5216\n",
            "Epoch 89, Loss: 0.5187\n",
            "Epoch 90, Loss: 0.5185\n",
            "Epoch 91, Loss: 0.5189\n",
            "Epoch 92, Loss: 0.5195\n",
            "Epoch 93, Loss: 0.5185\n",
            "Epoch 94, Loss: 0.5187\n",
            "Epoch 95, Loss: 0.5189\n",
            "Epoch 96, Loss: 0.5205\n",
            "Epoch 97, Loss: 0.5187\n",
            "Epoch 98, Loss: 0.5266\n",
            "Epoch 99, Loss: 0.5210\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_gd_with_regularization(X, y, learning_rate=0.01, epochs=1000, lambda_=0.1, reg_type='l2'):\n",
        "    theta = np.zeros(X.shape[1])\n",
        "    m = len(y)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        gradient = (1/m) * X.T.dot(X.dot(theta) - y)\n",
        "        if reg_type == 'l2':\n",
        "            gradient += lambda_ * theta  # L2 penalty\n",
        "        theta -= learning_rate * gradient\n",
        "        if epoch % 100 == 0:\n",
        "            loss = np.mean((X.dot(theta) - y) ** 2)\n",
        "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "    return theta\n",
        "\n",
        "theta_l2 = batch_gd_with_regularization(X_train, y_train, reg_type='l2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2Qw-KC9Axs4",
        "outputId": "0487542e-82e4-435c-c532-8c4e55aee11d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 5.5300\n",
            "Epoch 100, Loss: 1.3881\n",
            "Epoch 200, Loss: 0.8039\n",
            "Epoch 300, Loss: 0.6892\n",
            "Epoch 400, Loss: 0.6550\n",
            "Epoch 500, Loss: 0.6402\n",
            "Epoch 600, Loss: 0.6318\n",
            "Epoch 700, Loss: 0.6262\n",
            "Epoch 800, Loss: 0.6220\n",
            "Epoch 900, Loss: 0.6188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_gd_with_early_stopping(X, y, learning_rate=0.01, epochs=1000, patience=5):\n",
        "    theta = np.zeros(X.shape[1])\n",
        "    m = len(y)\n",
        "    best_loss = np.inf\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        gradient = (1/m) * X.T.dot(X.dot(theta) - y)\n",
        "        theta -= learning_rate * gradient\n",
        "        loss = np.mean((X.dot(theta) - y) ** 2)\n",
        "\n",
        "        if loss < best_loss:\n",
        "            best_loss = loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "    return theta\n",
        "\n",
        "theta_early_stop = batch_gd_with_early_stopping(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnaiFUyhAzO_",
        "outputId": "f8cccb29-2f4a-41ab-a69f-de3c0b76c567"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 5.5300\n",
            "Epoch 100, Loss: 1.2906\n",
            "Epoch 200, Loss: 0.7102\n",
            "Epoch 300, Loss: 0.6187\n",
            "Epoch 400, Loss: 0.5952\n",
            "Epoch 500, Loss: 0.5827\n",
            "Epoch 600, Loss: 0.5732\n",
            "Epoch 700, Loss: 0.5652\n",
            "Epoch 800, Loss: 0.5584\n",
            "Epoch 900, Loss: 0.5526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(theta, X_test, y_test):\n",
        "    predictions = X_test.dot(theta)\n",
        "    mse = np.mean((predictions - y_test) ** 2)\n",
        "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "\n",
        "print(\"Batch GD Performance:\")\n",
        "evaluate_model(theta_batch, X_test, y_test)\n",
        "\n",
        "print(\"\\nSGD Performance:\")\n",
        "evaluate_model(theta_sgd, X_test, y_test)\n",
        "\n",
        "print(\"\\nMini-Batch GD Performance:\")\n",
        "evaluate_model(theta_minibatch, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePxk-xNnA4Zo",
        "outputId": "c3d8754c-f66f-4eee-8d25-e2c6217db93c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch GD Performance:\n",
            "Mean Squared Error (MSE): 0.5672\n",
            "\n",
            "SGD Performance:\n",
            "Mean Squared Error (MSE): 16165022897498629495739415829484114686291783232081008015506160928527625595162409208392602391268243922001037672060521807872.0000\n",
            "\n",
            "Mini-Batch GD Performance:\n",
            "Mean Squared Error (MSE): 0.5769\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the model and scaler\n",
        "joblib.dump(theta_batch, 'batch_gd_model.pkl')\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "\n",
        "# Verify files are saved\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kI0wfCOgBQjZ",
        "outputId": "c6e14f39-d8a5-4574-87c8-faba7826e2bb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_gd_model.pkl  ml_env  sample_data  scaler.pkl\n"
          ]
        }
      ]
    }
  ]
}